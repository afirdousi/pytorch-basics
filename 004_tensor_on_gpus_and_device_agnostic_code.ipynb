{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOXH2VzPdFvNrrc6kcdtRE2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/afirdousi/pytorch-basics/blob/main/004_tensor_on_gpus_and_device_agnostic_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KS--eKAgOl6D",
        "outputId": "49a43d1d-04ee-4e44-c28c-f517d17d9ddb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello Pytorch World!\n",
            "2.0.1+cu118\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "print(\"Hello Pytorch World!\")\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Running tensor and PyTorch objects on GPUs makes the computation faster due to parallelism\n",
        "# Thanks to CUDA, Nvidia Hardware and Pytorch"
      ],
      "metadata": {
        "id": "3lG0JDy3OnMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Steps\n",
        "# 1. Getting a GPU\n",
        "# ---- Use Google Colab (Colab Pro or Pro+): Start here, good for quick learning and experimentation\n",
        "# ---- Use your own GPU  ( https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/ )\n",
        "# ---- Use cloud computing ( GCP, AWS, Azure)\n",
        "\n",
        "# ---- Running GPU + CUDA on local machine takes a bit of setup, check pytorch documentation"
      ],
      "metadata": {
        "id": "CG1bXMsvO7lN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For Colab\n",
        "# Goto Runtime -> Change Runtime Type --> Select GPU (You can T4 for free on Colab Free: https://www.nvidia.com/en-us/data-center/tesla-t4/)\n",
        "# Once done, run the following:\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hv70MbRqPxeC",
        "outputId": "fca5ba8e-5c1f-474a-cd34-13f5e7575cf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Aug 18 06:11:03 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   56C    P8    12W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if you have GPU access with PyTorch\n",
        "torch.cuda.is_available() # CUDA is Nvidia's programming interface over GPU"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dG8QNpFQzsS",
        "outputId": "f2b901f4-e67a-42a3-d817-5ea5bf242d1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup Device Agnostic Concept\n",
        "# You might not always have access to GPU but you want to use GPU whenever available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "upFSGgmSRHR9",
        "outputId": "540ff3c4-454f-4f66-eb6a-eab622b1cd91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# How to count the number of GPUs\n",
        "torch.cuda.device_count() # if you are running larger models and if you have multiple devices, you might want to run one model on each device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U__WKb9URdul",
        "outputId": "65c71863-8f40-4438-c572-52fce58f1962"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Learn more here: https://pytorch.org/docs/stable/notes/cuda.html\n",
        "# Check best practices for running Cuda: https://pytorch.org/docs/stable/notes/cuda.html#best-practices"
      ],
      "metadata": {
        "id": "oehMRu9tRiX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Putting tensors and models on GPU for faster computations\n",
        "tensor = torch.tensor([10,20,30,40,50], device='cpu')\n",
        "print(tensor, tensor.device)\n",
        "\n",
        "tensor = torch.tensor([10,20,30,40,50])\n",
        "print(tensor, tensor.device) # even by default, device = cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AZKnzMISF3t",
        "outputId": "11b69990-bbaf-4e8a-e50a-2df847ff58d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([10, 20, 30, 40, 50]) cpu\n",
            "tensor([10, 20, 30, 40, 50]) cpu\n",
            "CPU times: user 1.93 ms, sys: 0 ns, total: 1.93 ms\n",
            "Wall time: 1.94 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "tensor_on_gpu = tensor.to(device)\n",
        "tensor_on_gpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDxiHtWASpC8",
        "outputId": "abbc806a-3e05-4127-de90-6aa055913ce7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 192 µs, sys: 24 µs, total: 216 µs\n",
            "Wall time: 226 µs\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([10, 20, 30, 40, 50], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Moving tensors back to CPU\n",
        "# Tensors on GPU can't be transformed to NumPy\n",
        "\n",
        "# Run and recomment it\n",
        "# tensor_on_gpu.numpy() # TypeError: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.\n",
        "\n",
        "# Numpy doesn't work with GPU"
      ],
      "metadata": {
        "id": "10hLlr2AS7fe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How to fix GPU tensor to NumPy\n",
        "tensor_back_on_cpu = tensor_on_gpu.cpu().numpy()\n",
        "tensor_back_on_cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTCqyJDMTXKU",
        "outputId": "8215c0e4-640a-4425-fc86-b573d894df90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([10, 20, 30, 40, 50])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_on_gpu # this remains unchanged"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9h7iJBSToJZ",
        "outputId": "88ff447b-2597-4af1-8ddb-a57e8d5f7de0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([10, 20, 30, 40, 50], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Action: Learn how to run Pytorch to use multiple GPUs\n",
        "# https://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html\n",
        "\n",
        "# For practicing fundamentals of PyTorch: https://www.learnpytorch.io/00_pytorch_fundamentals/#exercises"
      ],
      "metadata": {
        "id": "57N038ZsTrqB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}